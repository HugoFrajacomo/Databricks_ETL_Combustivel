# ETL Pipeline - S√©ries Hist√≥ricas de Pre√ßos de Combust√≠veis (GOV.BR)

## Base de dados

### **Metadados da S√©rie Hist√≥rica de Pre√ßos de Combust√≠veis**

**Descri√ß√£o**
S√©rie Hist√≥rica de Pre√ßos de Combust√≠veis, com dados da pesquisa de pre√ßos da Ag√™ncia Nacional do Petr√≥leo, G√°s Natural e Biocombust√≠veis (ANP), em formato de dados abertos. A ANP, no desempenho de suas atribui√ß√µes legais, acompanha o comportamento de pre√ßos praticados pelas distribuidoras e postos revendedores de combust√≠veis por meio do Levantamento de Pre√ßos de Combust√≠veis (LPC).

**Fonte de Dados**

- **√ìrg√£o**: Ag√™ncia Nacional do Petr√≥leo, G√°s Natural e Biocombust√≠veis (ANP)
- **Formato**: CSVs semestrais (Hist√≥rico de pre√ßos)
- **Disponibilidade**: [Portal de Dados Abertos do Governo Federal](https://dados.gov.br/dados/conjuntos-dados/serie-historica-de-precos-de-combustiveis-e-de-glp)
- **Documenta√ß√£o**: [Informa√ß√µes sobre Levantamento de Pre√ßos de Combust√≠veis](https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/precos/precos-revenda-e-de-distribuicao-combustiveis/informacoes-levantamento-de-precos-de-combustiveis)

- **Dados utilizados**
Entre os dados dispon√≠veis na base da ANP, este projeto utilizou as s√©ries dos √∫ltimos 5 anos, com possibilidade de atualiza√ß√£o anual automatizada. Os dados foram restritos aos relat√≥rios semestrais de Combust√≠veis Automotivos.

---

### **Campos da Tabela**

| Campo | Descri√ß√£o do Atributo | Tipo | Fonte dos Dados |
|---|---|---|---|
| **Regi√£o - Sigla** | Sigla da Regi√£o onde a revenda pesquisada se encontra | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Estado - Sigla** | Sigla da Unidade Federativa (UF) da revenda pesquisada | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Munic√≠pio** | Nome do munic√≠pio da revenda pesquisada | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Revenda** | Nome do Cadastro Nacional de Pessoa Jur√≠dica da revenda | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |
| **CNPJ da Revenda**| Nome do logradouro da revenda pesquisada | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |
| **Nome da Rua** | N√∫mero do logradouro da revenda pesquisada | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |
| **Numero Rua** | Complemento do logradouro da revenda | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |
| **Complemento** | Nome do bairro da revenda pesquisada | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |
| **Bairro** | N√∫mero do C√≥digo de Endere√ßamento Postal (CEP) do logradouro | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Cep** | Nome do combust√≠vel | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Produto** | Data da coleta do pre√ßo | Data | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Valor de Venda**| Pre√ßo de venda ao consumidor final | Num√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Valor de Compra** | Pre√ßo de distribui√ß√£o (pre√ßo de venda da distribuidora para o posto revendedor) | Num√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC) |
| **Unidade de Medida** | Unidade de Medida | Alfanum√©rico | Levantamento de Pre√ßos de Combust√≠veis (LPC)  |
| **Bandeira** | Nome da bandeira da revenda. Posto que opta por exibir a marca comercial de uma distribuidora, dever√° vender somente combust√≠veis fornecidos pela distribuidora detentora da marca. Posto bandeira branca √© aquele que opta por n√£o exibir marca comercial. | Alfanum√©rico | Sistema de Movimenta√ß√£o de Produtos (SIMP) |

---

## Arquitetura do Projeto

### **Vis√£o Geral**

Este projeto implementa um pipeline ETL seguindo a **Medallion Architecture** para processamento e an√°lise de dados hist√≥ricos de pre√ßos de combust√≠veis. A arquitetura √© composta por tr√™s camadas principais: Bronze (dados brutos), Silver (dados limpos e transformados) e Gold (dados agregados e enriquecidos).

### **Tecnologias Utilizadas**

| Categoria | Tecnologia | Descri√ß√£o |
|---|---|---|
| **Linguagem de Programa√ß√£o** | Python | Linguagem principal para desenvolvimento dos pipelines ETL |
| **Framework de Processamento** | Apache Spark | Engine distribu√≠da para processamento de big data em larga escala |
| **Plataforma de Dados** | Databricks | Plataforma unificada de analytics baseada em Apache Spark |
| **Formato de Dados** | Delta Lake | Formato de armazenamento para data lakes com versionamento e ACID |
| **Controle de Vers√£o** | Git | Sistema de controle de vers√£o distribu√≠do para c√≥digo e notebooks |
| **Reposit√≥rio** | GitHub | Plataforma de hospedagem e colabora√ß√£o de c√≥digo |
| **Notebooks** | Databricks Notebooks | Ambiente interativo para desenvolvimento e documenta√ß√£o |
| **Visualiza√ß√£o** | Databricks Dashboards | Dashboards nativos para visualiza√ß√£o de dados |
| **IA Conversacional** | Databricks Genie | Interface de consulta em linguagem natural |

#### **Vantagens**

- **Python + Apache Spark**:
Permite o desenvolvimento de pipelines ETL escal√°veis e eficientes, aproveitando a simplicidade do Python e o poder de processamento distribu√≠do do Spark. Ideal para manipula√ß√£o de grandes volumes de dados, transforma√ß√µes complexas e integra√ß√£o com bibliotecas anal√≠ticas.

- **Databricks**:
Plataforma colaborativa que integra processamento distribu√≠do, notebooks interativos e gerenciamento de clusters. Facilita o desenvolvimento, execu√ß√£o e monitoramento de fluxos de dados, al√©m de oferecer integra√ß√£o nativa com Spark e Delta Lake.

- **Delta Tables**:
Formato otimizado para data lakes, com suporte a transa√ß√µes ACID, versionamento e auditoria de dados. Garante integridade, rastreabilidade e alta performance em opera√ß√µes de leitura e escrita, essenciais para o tratamento confi√°vel de dados.

- **Databricks Dashboards**:
Permite a cria√ß√£o r√°pida de visualiza√ß√µes interativas diretamente dos notebooks, facilitando o acompanhamento dos resultados do tratamento de dados e a comunica√ß√£o com √°reas de neg√≥cio.

- **Databricks Genie**:
Interface de consulta em linguagem natural que agiliza o acesso e an√°lise dos dados tratados, tornando o processo de explora√ß√£o e gera√ß√£o de insights mais intuitivo e acess√≠vel para usu√°rios n√£o t√©cnicos.

### **Camadas da Arquitetura**

A **Medallion Architecture** √© um padr√£o de design para data lakes que organiza os dados em tr√™s camadas progressivas de qualidade e refinamento, garantindo flexibilidade, escalabilidade e governan√ßa de dados.

#### **ü•â Bronze Layer - Ingest√£o Bruta**
**Conceito**: Camada de armazenamento de dados brutos, exatamente como recebidos da fonte, sem qualquer transforma√ß√£o. Atua como um "backup" dos dados originais, preservando a integridade e permitindo reprocessamento quando necess√°rio.

- **Objetivo**: Armazenamento de dados brutos sem transforma√ß√µes
- **Implementa√ß√£o**: 
  - Job Databricks para download autom√°tico de arquivos CSV
  - Armazenamento no Databricks Catalog
  - Formato: Delta Tables
- **Caracter√≠sticas**:
  - Preserva√ß√£o dos dados originais
  - Versionamento de dados
  - Auditoria completa

#### **ü•à Silver Layer - Transforma√ß√£o e Limpeza**
**Conceito**: Camada intermedi√°ria onde os dados passam por processos de limpeza, valida√ß√£o e padroniza√ß√£o. Transforma dados brutos em datasets confi√°veis e consistentes, servindo como base para an√°lises e transforma√ß√µes posteriores.

- **Objetivo**: Padroniza√ß√£o e limpeza dos dados
- **Transforma√ß√µes**:
  - Convers√£o de tipos de dados
  - Padroniza√ß√£o de nomes de colunas
  - Remo√ß√£o de registros duplicados
  - Valida√ß√£o de dados
- **Qualidade**: Dados confi√°veis e estruturados para an√°lise

#### **ü•á Gold Layer - Enriquecimento e Agrega√ß√µes**
**Conceito**: Camada final que cont√©m dados refinados, agregados e otimizados para consumo espec√≠fico. Fornece datasets prontos para an√°lises de neg√≥cio, relat√≥rios e com alta performance e usabilidade.

- **Objetivo**: Dados otimizados para consumo anal√≠tico
- **Agrega√ß√µes**:
  - Pre√ßo m√©dio por cidade/estado/semana
  - √çndices de varia√ß√£o (% mensal, % anual)
  - An√°lises temporais e geogr√°ficas
- **Prepara√ß√£o**: Tabelas otimizadas para BI e Machine Learning

### **Arquitetura das tabelas**
O projeto utiliza a arquitetura star schema, que organiza os dados em uma estrutura centralizada e otimizada para an√°lise. Neste modelo, as tabelas dimens√£o fornecem contexto e detalhamento para a tabela fato, facilitando consultas anal√≠ticas e agrega√ß√µes.

As tabelas constru√≠das incluem:
- **Dimens√£o Produtos**: cont√©m os campos `Produto` (tipo de combust√≠vel) e `UnidadeMedida` (unidade de comercializa√ß√£o), permitindo detalhar as caracter√≠sticas dos combust√≠veis presentes na base.
- **Dimens√£o Tempo**: composta por `DataColeta` (data da coleta), `dia`, `mes`, `ano`, `semana` e `trimestre`, possibilitando an√°lises temporais em diferentes granularidades.
- **Dimens√£o Localidade**: inclui `Cep` (c√≥digo postal), `RegiaoSigla` (regi√£o), `EstadoSigla` (UF) e `Municipio`, fornecendo contexto geogr√°fico para os dados de revenda e consumo.
- **Dimens√£o Revenda**: abrange `CNPJRevenda` (identificador do posto), `Revenda` (nome), `NomeRua`, `NumeroRua`, `Complemento`, `Bandeira` e `cep`, detalhando a localiza√ß√£o e caracter√≠sticas dos postos revendedores.
- **Tabela Fato de Combust√≠veis**: registra os dados de transa√ß√µes, com as colunas `CNPJRevenda`, `Cep`, `Produto`, `DataColeta`, `ValorVenda` e `ValorCompra`, conectando-se √†s dimens√µes por meio de chaves e permitindo an√°lises detalhadas e cruzamentos entre produto, tempo, localidade e revenda.

### **Consumo e Visualiza√ß√£o**

#### **Business Intelligence**
- **Dashboard Databricks**: Visualiza√ß√µes interativas e relat√≥rios
- **Databricks Genie**: Interface de consulta em linguagem natural

### **Estrutura do Projeto**

```
Databricks_ETL_Combustivel/
‚îú‚îÄ‚îÄ 00_Architecture_Builder/     # Configura√ß√£o do cat√°logo e esquemas
‚îú‚îÄ‚îÄ 01_Bronze_Ingestion/        # Ingest√£o de dados brutos
‚îú‚îÄ‚îÄ 02_Silver_Treatment/        # Transforma√ß√£o, limpeza, fato e dimens√£o
‚îú‚îÄ‚îÄ 03_Gold_Enrichment/         # Agrega√ß√µes e enriquecimento
‚îî‚îÄ‚îÄ 04_BI/                      # Dashboards e visualiza√ß√µes

```

---

# Como utilizar o reposit√≥rio no Databricks Free Edition

1. **Crie uma conta gratuita no Databricks**
  - Acesse https://www.databricks.com/learn/free-edition e registre-se na op√ß√£o Sign up.

2. **Fa√ßa login e crie um workspace**
  - Ap√≥s o login, acesse o workspace padr√£o da Free Edition.

3. **Importe os notebooks do reposit√≥rio**
  - No menu lateral, clique em "Workspace" > "Users" > Seu usu√°rio.
  - Clique em "Import" e selecione os arquivos `.ipynb` do reposit√≥rio para importar os notebooks.


4. **Implemente o pipeline ETL automatizado**
  - Configure o pipeline ETL utilizando o recurso de Jobs do Databricks, conforme o exemplo abaixo:

```yaml
resources:
  jobs:
    ETL_Pipeline:
      name: ETL Pipeline
      email_notifications:
        on_start:
          - seu_email@dominio.com
        on_success:
          - seu_email@dominio.com
        on_failure:
          - seu_email@dominio.com
      schedule:
        quartz_cron_expression: 34 0 5 1 * ?
        timezone_id: America/Sao_Paulo
        pause_status: UNPAUSED
      tasks:
        - task_key: Arch_build
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/00_Architectury_Builder/catalog_build
            source: WORKSPACE
        - task_key: Bronze_Ingestion
          depends_on:
            - task_key: Arch_build
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/01_Bronze_Ingestion/gov_br_data
            source: WORKSPACE
        - task_key: Silver_data_before_2020
          depends_on:
            - task_key: Bronze_Ingestion
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/01_tables_before_2020
            source: WORKSPACE
        - task_key: Join_full_Bronze_Tables
          depends_on:
            - task_key: Silver_data_before_2020
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/02_enrichment_fuel_data
            source: WORKSPACE
        - task_key: Build_Dim_Localidade
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/05_dim_localidade
            source: WORKSPACE
        - task_key: Build_Dim_Product
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/03_dim_produtos
            source: WORKSPACE
        - task_key: Build_Dim_Revenda
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/06_dim_revenda
            source: WORKSPACE
        - task_key: Build_Dim_tempo
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/04_dim_tempo
            source: WORKSPACE
        - task_key: Set_Pk_Dim_Tables
          depends_on:
            - task_key: Build_Dim_Localidade
            - task_key: Build_Dim_Product
            - task_key: Build_Dim_Revenda
            - task_key: Build_Dim_tempo
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/07_ajuste_pks
            source: WORKSPACE
        - task_key: Build_Fact_Table
          depends_on:
            - task_key: Set_Pk_Dim_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/08_fact_fuel
            source: WORKSPACE
        - task_key: ComparacoesGeograficas
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/03_comparacoes_geograficas
            source: WORKSPACE
        - task_key: IndicadoresTemporais
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/02_indicadores_temporais
            source: WORKSPACE
        - task_key: infoProdutos
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/01_info_preco_produto
            source: WORKSPACE
        - task_key: InfoBandeira
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/04_Bandeira
            source: WORKSPACE
      queue:
        enabled: true
      performance_target: STANDARD
```

  > **Aten√ß√£o:**
  > - Substitua `seu_email@dominio.com` pelo(s) e-mail(s) que devem receber notifica√ß√µes do job.
  > - Altere `SEU_USUARIO` para o nome do seu usu√°rio Databricks.
  > - Ajuste os caminhos dos notebooks conforme a estrutura do seu workspace.
  > - Complete outros campos conforme a necessidade do seu ambiente.

  O exemplo acima mostra como orquestrar a execu√ß√£o dos notebooks do projeto em etapas dependentes, com notifica√ß√µes e agendamento autom√°tico. Basta adaptar os caminhos dos notebooks e configurar o job no Databricks para automatizar todo o pipeline ETL.


## **Configurando job runs pela interface**

Tamb√©m √© poss√≠vel configurar a pipeline ETL diretamente pela interface gr√°fica do Databricks. Siga os passos abaixo:

1. No menu lateral, acesse **Jobs Runs**.
2. Clique em **Create Job** para iniciar a configura√ß√£o de um novo job.
3. Para cada etapa da pipeline, adicione uma tarefa (**Add Task**) e selecione o notebook correspondente.
4. **Aten√ß√£o √† ordem dos notebooks:**
  - Os notebooks e pastas do projeto est√£o nomeados sequencialmente (00, 01, 02, ...).
  - Siga essa ordem ao adicionar as tarefas no job, garantindo que cada etapa dependa da anterior.
  - Por exemplo, configure o notebook de ingest√£o (01_Bronze_Ingestion) para ser executado ap√≥s o de arquitetura (00_Architectury_Builder), e assim por diante.
5. Para definir depend√™ncias entre tarefas, utilize a op√ß√£o **Depends on** ao adicionar cada notebook.
6. Configure notifica√ß√µes, agendamento e outros par√¢metros conforme sua necessidade.

> **Importante:**
> - A sequ√™ncia dos notebooks reflete a ordem l√≥gica de processamento dos dados. Seguir essa ordem √© fundamental para garantir que arquivos que dependem de outros sejam executados corretamente.
> - Caso altere nomes ou a estrutura das pastas, ajuste a configura√ß√£o do job para refletir essas mudan√ßas.

Ao finalizar, salve e execute o job para automatizar o pipeline ETL conforme a arquitetura do projeto.

6. **Visualize os resultados**
  - Utilize os dashboards e queries criados para explorar os dados tratados.

**Observa√ß√µes:**
- A free edition possui limita√ß√µes de recursos e armazenamento, mas permite testar todo o pipeline ETL e visualizar os resultados.
- Caso utilize outro ambiente Databricks, o processo de importa√ß√£o e execu√ß√£o √© similar, podendo aproveitar recursos adicionais.



# Como utilizar o reposit√≥rio no Azure Databricks

Para utilizar este reposit√≥rio no Azure Databricks, √© necess√°rio configurar um armazenamento externo para os dados e arquivos do pipeline. Recomenda-se a cria√ß√£o de um **Azure Blob Storage** para servir como base do DBFS (Databricks File System), permitindo o armazenamento e acesso eficiente aos dados.

**Passos principais:**

1. Crie uma conta de armazenamento do tipo Blob no portal do Azure.
2. Crie um container para armazenar os arquivos de dados e resultados do pipeline.
3. No Azure Databricks, monte o Blob Storage como DBFS utilizando as credenciais de acesso (SAS Token ou chave de acesso).
4. Importe os notebooks do reposit√≥rio normalmente e ajuste os caminhos de leitura/escrita para utilizar o DBFS montado.

> **Aten√ß√£o:**
> - O uso do DBFS via Blob Storage √© essencial para garantir persist√™ncia, escalabilidade e integra√ß√£o nativa com os recursos do Azure Databricks.
> - Certifique-se de que as permiss√µes de acesso ao Blob Storage estejam corretamente configuradas para leitura e escrita.
> - Adapte os notebooks e jobs para utilizar os caminhos do DBFS conforme o ambiente configurado.


Com essa configura√ß√£o, o pipeline ETL poder√° ser executado de forma automatizada e segura no Azure Databricks, aproveitando todos os recursos de armazenamento e processamento distribu√≠do da plataforma.

---

## Diferen√ßas entre Databricks Free Edition e Azure Databricks

| Recurso/Caracter√≠stica         | Free Edition Databricks           | Azure Databricks                       |
|-------------------------------|-----------------------------------|----------------------------------------|
| **Armazenamento**             | Limitado ao DBFS local, sem Blob  | Integra√ß√£o nativa com Azure Blob, ADLS |
| **Escalabilidade**            | Recursos computacionais restritos  | Clusters escal√°veis sob demanda        |
| **Seguran√ßa**                 | B√°sica, sem integra√ß√£o corporativa | Integra√ß√£o com Azure AD, RBAC, redes   |
| **Integra√ß√£o**                | Sem integra√ß√£o com outros servi√ßos | Integra√ß√£o total com Azure (Data Lake, Synapse, Key Vault, etc.) |
| **Persist√™ncia de Dados**     | Dados podem ser apagados           | Persist√™ncia garantida em Blob/ADLS    |
| **Agendamento de Jobs**       | Limitado, sem triggers avan√ßadas   | Agendamento flex√≠vel e triggers        |
| **Limite de Usu√°rios**        | Apenas 1 usu√°rio                   | Multiusu√°rio, colabora√ß√£o corporativa  |
| **Suporte e SLA**             | Sem suporte oficial                | Suporte Microsoft, SLA corporativo     |
| **Recursos Avan√ßados**        | N√£o dispon√≠vel                     | MLflow, Delta Sharing, Unity Catalog   |

> **Resumo:**
> - O Free Edition √© ideal para testes, prototipagem e aprendizado, mas possui limita√ß√µes de recursos, armazenamento e integra√ß√£o.
> - O Azure Databricks √© recomendado para ambientes produtivos, com escalabilidade, seguran√ßa, integra√ß√£o corporativa e persist√™ncia de dados garantida.


