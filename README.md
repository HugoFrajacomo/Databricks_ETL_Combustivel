# ETL Pipeline - SÃ©ries HistÃ³ricas de PreÃ§os de CombustÃ­veis (GOV.BR)

## Base de dados

### **Metadados da SÃ©rie HistÃ³rica de PreÃ§os de CombustÃ­veis**

**DescriÃ§Ã£o**
SÃ©rie HistÃ³rica de PreÃ§os de CombustÃ­veis, com dados da pesquisa de preÃ§os da AgÃªncia Nacional do PetrÃ³leo, GÃ¡s Natural e BiocombustÃ­veis (ANP), em formato de dados abertos. A ANP, no desempenho de suas atribuiÃ§Ãµes legais, acompanha o comportamento de preÃ§os praticados pelas distribuidoras e postos revendedores de combustÃ­veis por meio do Levantamento de PreÃ§os de CombustÃ­veis (LPC).

**Fonte de Dados**

- **Ã“rgÃ£o**: AgÃªncia Nacional do PetrÃ³leo, GÃ¡s Natural e BiocombustÃ­veis (ANP)
- **Formato**: CSVs semestrais (HistÃ³rico de preÃ§os)
- **Disponibilidade**: [Portal de Dados Abertos do Governo Federal](https://dados.gov.br/dados/conjuntos-dados/serie-historica-de-precos-de-combustiveis-e-de-glp)
- **DocumentaÃ§Ã£o**: [InformaÃ§Ãµes sobre Levantamento de PreÃ§os de CombustÃ­veis](https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/precos/precos-revenda-e-de-distribuicao-combustiveis/informacoes-levantamento-de-precos-de-combustiveis)

- **Dados utilizados**
Entre os dados disponÃ­veis na base da ANP, este projeto utilizou as sÃ©ries dos Ãºltimos 5 anos, com possibilidade de atualizaÃ§Ã£o anual automatizada. Os dados foram restritos aos relatÃ³rios semestrais de CombustÃ­veis Automotivos.

---

### **Campos da Tabela**

| Campo | DescriÃ§Ã£o do Atributo | Tipo | Fonte dos Dados |
|---|---|---|---|
| **RegiÃ£o - Sigla** | Sigla da RegiÃ£o onde a revenda pesquisada se encontra | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Estado - Sigla** | Sigla da Unidade Federativa (UF) da revenda pesquisada | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **MunicÃ­pio** | Nome do municÃ­pio da revenda pesquisada | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Revenda** | Nome do Cadastro Nacional de Pessoa JurÃ­dica da revenda | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |
| **CNPJ da Revenda**| Nome do logradouro da revenda pesquisada | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |
| **Nome da Rua** | NÃºmero do logradouro da revenda pesquisada | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |
| **Numero Rua** | Complemento do logradouro da revenda | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |
| **Complemento** | Nome do bairro da revenda pesquisada | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |
| **Bairro** | NÃºmero do CÃ³digo de EndereÃ§amento Postal (CEP) do logradouro | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Cep** | Nome do combustÃ­vel | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Produto** | Data da coleta do preÃ§o | Data | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Valor de Venda**| PreÃ§o de venda ao consumidor final | NumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Valor de Compra** | PreÃ§o de distribuiÃ§Ã£o (preÃ§o de venda da distribuidora para o posto revendedor) | NumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC) |
| **Unidade de Medida** | Unidade de Medida | AlfanumÃ©rico | Levantamento de PreÃ§os de CombustÃ­veis (LPC)  |
| **Bandeira** | Nome da bandeira da revenda. Posto que opta por exibir a marca comercial de uma distribuidora, deverÃ¡ vender somente combustÃ­veis fornecidos pela distribuidora detentora da marca. Posto bandeira branca Ã© aquele que opta por nÃ£o exibir marca comercial. | AlfanumÃ©rico | Sistema de MovimentaÃ§Ã£o de Produtos (SIMP) |

---

## Arquitetura do Projeto

### **VisÃ£o Geral**

Este projeto implementa um pipeline ETL seguindo a **Medallion Architecture** para processamento e anÃ¡lise de dados histÃ³ricos de preÃ§os de combustÃ­veis. A arquitetura Ã© composta por trÃªs camadas principais: Bronze (dados brutos), Silver (dados limpos e transformados) e Gold (dados agregados e enriquecidos).

### **Tecnologias Utilizadas**

| Categoria | Tecnologia | DescriÃ§Ã£o |
|---|---|---|
| **Linguagem de ProgramaÃ§Ã£o** | Python | Linguagem principal para desenvolvimento dos pipelines ETL |
| **Framework de Processamento** | Apache Spark | Engine distribuÃ­da para processamento de big data em larga escala |
| **Plataforma de Dados** | Databricks | Plataforma unificada de analytics baseada em Apache Spark |
| **Formato de Dados** | Delta Lake | Formato de armazenamento para data lakes com versionamento e ACID |
| **Controle de VersÃ£o** | Git | Sistema de controle de versÃ£o distribuÃ­do para cÃ³digo e notebooks |
| **RepositÃ³rio** | GitHub | Plataforma de hospedagem e colaboraÃ§Ã£o de cÃ³digo |
| **Notebooks** | Databricks Notebooks | Ambiente interativo para desenvolvimento e documentaÃ§Ã£o |
| **VisualizaÃ§Ã£o** | Databricks Dashboards | Dashboards nativos para visualizaÃ§Ã£o de dados |
| **IA Conversacional** | Databricks Genie | Interface de consulta em linguagem natural |

#### **Vantagens**

- **Python + Apache Spark**:
Permite o desenvolvimento de pipelines ETL escalÃ¡veis e eficientes, aproveitando a simplicidade do Python e o poder de processamento distribuÃ­do do Spark. Ideal para manipulaÃ§Ã£o de grandes volumes de dados, transformaÃ§Ãµes complexas e integraÃ§Ã£o com bibliotecas analÃ­ticas.

- **Databricks**:
Plataforma colaborativa que integra processamento distribuÃ­do, notebooks interativos e gerenciamento de clusters. Facilita o desenvolvimento, execuÃ§Ã£o e monitoramento de fluxos de dados, alÃ©m de oferecer integraÃ§Ã£o nativa com Spark e Delta Lake.

- **Delta Tables**:
Formato otimizado para data lakes, com suporte a transaÃ§Ãµes ACID, versionamento e auditoria de dados. Garante integridade, rastreabilidade e alta performance em operaÃ§Ãµes de leitura e escrita, essenciais para o tratamento confiÃ¡vel de dados.

- **Databricks Dashboards**:
Permite a criaÃ§Ã£o rÃ¡pida de visualizaÃ§Ãµes interativas diretamente dos notebooks, facilitando o acompanhamento dos resultados do tratamento de dados e a comunicaÃ§Ã£o com Ã¡reas de negÃ³cio.

- **Databricks Genie**:
Interface de consulta em linguagem natural que agiliza o acesso e anÃ¡lise dos dados tratados, tornando o processo de exploraÃ§Ã£o e geraÃ§Ã£o de insights mais intuitivo e acessÃ­vel para usuÃ¡rios nÃ£o tÃ©cnicos.

### **Camadas da Arquitetura**

A **Medallion Architecture** Ã© um padrÃ£o de design para data lakes que organiza os dados em trÃªs camadas progressivas de qualidade e refinamento, garantindo flexibilidade, escalabilidade e governanÃ§a de dados.

#### **ğŸ¥‰ Bronze Layer - IngestÃ£o Bruta**
**Conceito**: Camada de armazenamento de dados brutos, exatamente como recebidos da fonte, sem qualquer transformaÃ§Ã£o. Atua como um "backup" dos dados originais, preservando a integridade e permitindo reprocessamento quando necessÃ¡rio.

- **Objetivo**: Armazenamento de dados brutos sem transformaÃ§Ãµes
- **ImplementaÃ§Ã£o**: 
  - Job Databricks para download automÃ¡tico de arquivos CSV
  - Armazenamento no Databricks Catalog
  - Formato: Delta Tables
- **CaracterÃ­sticas**:
  - PreservaÃ§Ã£o dos dados originais
  - Versionamento de dados
  - Auditoria completa

#### **ğŸ¥ˆ Silver Layer - TransformaÃ§Ã£o e Limpeza**
**Conceito**: Camada intermediÃ¡ria onde os dados passam por processos de limpeza, validaÃ§Ã£o e padronizaÃ§Ã£o. Transforma dados brutos em datasets confiÃ¡veis e consistentes, servindo como base para anÃ¡lises e transformaÃ§Ãµes posteriores.

- **Objetivo**: PadronizaÃ§Ã£o e limpeza dos dados
- **TransformaÃ§Ãµes**:
  - ConversÃ£o de tipos de dados
  - PadronizaÃ§Ã£o de nomes de colunas
  - RemoÃ§Ã£o de registros duplicados
  - ValidaÃ§Ã£o de dados
- **Qualidade**: Dados confiÃ¡veis e estruturados para anÃ¡lise

#### **ğŸ¥‡ Gold Layer - Enriquecimento e AgregaÃ§Ãµes**
**Conceito**: Camada final que contÃ©m dados refinados, agregados e otimizados para consumo especÃ­fico. Fornece datasets prontos para anÃ¡lises de negÃ³cio, relatÃ³rios e com alta performance e usabilidade.

- **Objetivo**: Dados otimizados para consumo analÃ­tico
- **AgregaÃ§Ãµes**:
  - PreÃ§o mÃ©dio por cidade/estado/semana
  - Ãndices de variaÃ§Ã£o (% mensal, % anual)
  - AnÃ¡lises temporais e geogrÃ¡ficas
- **PreparaÃ§Ã£o**: Tabelas otimizadas para BI e Machine Learning

### **Arquitetura das tabelas**
O projeto utiliza a arquitetura star schema, que organiza os dados em uma estrutura centralizada e otimizada para anÃ¡lise. Neste modelo, as tabelas dimensÃ£o fornecem contexto e detalhamento para a tabela fato, facilitando consultas analÃ­ticas e agregaÃ§Ãµes.

As tabelas construÃ­das incluem:
- **DimensÃ£o Produtos**: contÃ©m os campos `Produto` (tipo de combustÃ­vel) e `UnidadeMedida` (unidade de comercializaÃ§Ã£o), permitindo detalhar as caracterÃ­sticas dos combustÃ­veis presentes na base.
- **DimensÃ£o Tempo**: composta por `DataColeta` (data da coleta), `dia`, `mes`, `ano`, `semana` e `trimestre`, possibilitando anÃ¡lises temporais em diferentes granularidades.
- **DimensÃ£o Localidade**: inclui `Cep` (cÃ³digo postal), `RegiaoSigla` (regiÃ£o), `EstadoSigla` (UF) e `Municipio`, fornecendo contexto geogrÃ¡fico para os dados de revenda e consumo.
- **DimensÃ£o Revenda**: abrange `CNPJRevenda` (identificador do posto), `Revenda` (nome), `NomeRua`, `NumeroRua`, `Complemento`, `Bandeira` e `cep`, detalhando a localizaÃ§Ã£o e caracterÃ­sticas dos postos revendedores.
- **Tabela Fato de CombustÃ­veis**: registra os dados de transaÃ§Ãµes, com as colunas `CNPJRevenda`, `Cep`, `Produto`, `DataColeta`, `ValorVenda` e `ValorCompra`, conectando-se Ã s dimensÃµes por meio de chaves e permitindo anÃ¡lises detalhadas e cruzamentos entre produto, tempo, localidade e revenda.

### **Consumo e VisualizaÃ§Ã£o**

#### **Business Intelligence**
- **Dashboard Databricks**: VisualizaÃ§Ãµes interativas e relatÃ³rios
- **Databricks Genie**: Interface de consulta em linguagem natural

### **Estrutura do Projeto**

```
Databricks_ETL_Combustivel/
â”œâ”€â”€ 00_Architecture_Builder/     # ConfiguraÃ§Ã£o do catÃ¡logo e esquemas
â”œâ”€â”€ 01_Bronze_Ingestion/        # IngestÃ£o de dados brutos
â”œâ”€â”€ 02_Silver_Treatment/        # TransformaÃ§Ã£o, limpeza, fato e dimensÃ£o
â”œâ”€â”€ 03_Gold_Enrichment/         # AgregaÃ§Ãµes e enriquecimento
â””â”€â”€ 04_BI/                      # Dashboards e visualizaÃ§Ãµes

```

---

# Como utilizar o repositÃ³rio no Databricks Free Edition

1. **Crie uma conta gratuita no Databricks**
  - Acesse https://www.databricks.com/learn/free-edition e registre-se na opÃ§Ã£o Sign up.

2. **FaÃ§a login e crie um workspace**
  - ApÃ³s o login, acesse o workspace padrÃ£o da Free Edition.

3. **Importe os notebooks do repositÃ³rio**
  - No menu lateral, clique em "Workspace" > "Users" > Seu usuÃ¡rio.
  - Clique em "Import" e selecione os arquivos `.ipynb` do repositÃ³rio para importar os notebooks.


4. **Implemente o pipeline ETL automatizado**
  - Configure o pipeline ETL utilizando o recurso de Jobs do Databricks, conforme o exemplo abaixo:

```yaml
resources:
  jobs:
    ETL_Pipeline:
      name: ETL Pipeline
      email_notifications:
        on_start:
          - seu_email@dominio.com
        on_success:
          - seu_email@dominio.com
        on_failure:
          - seu_email@dominio.com
      schedule:
        quartz_cron_expression: 34 0 5 1 * ?
        timezone_id: America/Sao_Paulo
        pause_status: UNPAUSED
      tasks:
        - task_key: Arch_build
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/00_Architectury_Builder/catalog_build
            source: WORKSPACE
        - task_key: Bronze_Ingestion
          depends_on:
            - task_key: Arch_build
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/01_Bronze_Ingestion/gov_br_data
            source: WORKSPACE
        - task_key: Silver_data_before_2020
          depends_on:
            - task_key: Bronze_Ingestion
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/01_tables_before_2020
            source: WORKSPACE
        - task_key: Join_full_Bronze_Tables
          depends_on:
            - task_key: Silver_data_before_2020
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/02_enrichment_fuel_data
            source: WORKSPACE
        - task_key: Build_Dim_Localidade
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/05_dim_localidade
            source: WORKSPACE
        - task_key: Build_Dim_Product
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/03_dim_produtos
            source: WORKSPACE
        - task_key: Build_Dim_Revenda
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/06_dim_revenda
            source: WORKSPACE
        - task_key: Build_Dim_tempo
          depends_on:
            - task_key: Join_full_Bronze_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/04_dim_tempo
            source: WORKSPACE
        - task_key: Set_Pk_Dim_Tables
          depends_on:
            - task_key: Build_Dim_Localidade
            - task_key: Build_Dim_Product
            - task_key: Build_Dim_Revenda
            - task_key: Build_Dim_tempo
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/07_ajuste_pks
            source: WORKSPACE
        - task_key: Build_Fact_Table
          depends_on:
            - task_key: Set_Pk_Dim_Tables
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/02_Silver_Treatment/08_fact_fuel
            source: WORKSPACE
        - task_key: ComparacoesGeograficas
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/03_comparacoes_geograficas
            source: WORKSPACE
        - task_key: IndicadoresTemporais
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/02_indicadores_temporais
            source: WORKSPACE
        - task_key: infoProdutos
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/01_info_preco_produto
            source: WORKSPACE
        - task_key: InfoBandeira
          depends_on:
            - task_key: Build_Fact_Table
          notebook_task:
            notebook_path: /Workspace/Users/SEU_USUARIO/Databricks_ETL_Combustivel/03_Gold_Enrichment/04_Bandeira
            source: WORKSPACE
      queue:
        enabled: true
      performance_target: STANDARD
```

  > **AtenÃ§Ã£o:**
  > - Substitua `seu_email@dominio.com` pelo(s) e-mail(s) que devem receber notificaÃ§Ãµes do job.
  > - Altere `SEU_USUARIO` para o nome do seu usuÃ¡rio Databricks.
  > - Ajuste os caminhos dos notebooks conforme a estrutura do seu workspace.
  > - Complete outros campos conforme a necessidade do seu ambiente.

  O exemplo acima mostra como orquestrar a execuÃ§Ã£o dos notebooks do projeto em etapas dependentes, com notificaÃ§Ãµes e agendamento automÃ¡tico. Basta adaptar os caminhos dos notebooks e configurar o job no Databricks para automatizar todo o pipeline ETL.


## **Configurando job runs pela interface**

TambÃ©m Ã© possÃ­vel configurar a pipeline ETL diretamente pela interface grÃ¡fica do Databricks. Siga os passos abaixo:

1. No menu lateral, acesse **Jobs Runs**.
2. Clique em **Create Job** para iniciar a configuraÃ§Ã£o de um novo job.
3. Para cada etapa da pipeline, adicione uma tarefa (**Add Task**) e selecione o notebook correspondente.
4. **AtenÃ§Ã£o Ã  ordem dos notebooks:**
  - Os notebooks e pastas do projeto estÃ£o nomeados sequencialmente (00, 01, 02, ...).
  - Siga essa ordem ao adicionar as tarefas no job, garantindo que cada etapa dependa da anterior.
  - Por exemplo, configure o notebook de ingestÃ£o (01_Bronze_Ingestion) para ser executado apÃ³s o de arquitetura (00_Architectury_Builder), e assim por diante.
5. Para definir dependÃªncias entre tarefas, utilize a opÃ§Ã£o **Depends on** ao adicionar cada notebook.
6. Configure notificaÃ§Ãµes, agendamento e outros parÃ¢metros conforme sua necessidade.

> **Importante:**
> - A sequÃªncia dos notebooks reflete a ordem lÃ³gica de processamento dos dados. Seguir essa ordem Ã© fundamental para garantir que arquivos que dependem de outros sejam executados corretamente.
> - Caso altere nomes ou a estrutura das pastas, ajuste a configuraÃ§Ã£o do job para refletir essas mudanÃ§as.

Ao finalizar, salve e execute o job para automatizar o pipeline ETL conforme a arquitetura do projeto.

6. **Visualize os resultados**
  - Utilize os dashboards e queries criados para explorar os dados tratados.

**ObservaÃ§Ãµes:**
- A free edition possui limitaÃ§Ãµes de recursos e armazenamento, mas permite testar todo o pipeline ETL e visualizar os resultados.
- Caso utilize outro ambiente Databricks, o processo de importaÃ§Ã£o e execuÃ§Ã£o Ã© similar, podendo aproveitar recursos adicionais.



# Como utilizar o repositÃ³rio no Azure Databricks

Para utilizar este repositÃ³rio no Azure Databricks, Ã© necessÃ¡rio configurar um armazenamento externo para os dados e arquivos do pipeline. Recomenda-se a criaÃ§Ã£o de um **Azure Blob Storage** para servir como base do DBFS (Databricks File System), permitindo o armazenamento e acesso eficiente aos dados.

**Passos principais:**

1. Crie uma conta de armazenamento do tipo Blob no portal do Azure.
2. Crie um container para armazenar os arquivos de dados e resultados do pipeline.
3. No Azure Databricks, monte o Blob Storage como DBFS utilizando as credenciais de acesso (SAS Token ou chave de acesso).
4. Importe os notebooks do repositÃ³rio normalmente e ajuste os caminhos de leitura/escrita para utilizar o DBFS montado.

> **AtenÃ§Ã£o:**
> - O uso do DBFS via Blob Storage Ã© essencial para garantir persistÃªncia, escalabilidade e integraÃ§Ã£o nativa com os recursos do Azure Databricks.
> - Certifique-se de que as permissÃµes de acesso ao Blob Storage estejam corretamente configuradas para leitura e escrita.
> - Adapte os notebooks e jobs para utilizar os caminhos do DBFS conforme o ambiente configurado.


Com essa configuraÃ§Ã£o, o pipeline ETL poderÃ¡ ser executado de forma automatizada e segura no Azure Databricks, aproveitando todos os recursos de armazenamento e processamento distribuÃ­do da plataforma.

---

## DiferenÃ§as entre Databricks Free Edition e Azure Databricks

| Recurso/CaracterÃ­stica         | Free Edition Databricks           | Azure Databricks                       |
|-------------------------------|-----------------------------------|----------------------------------------|
| **Armazenamento**             | Limitado ao DBFS local, sem Blob  | IntegraÃ§Ã£o nativa com Azure Blob, ADLS |
| **Escalabilidade**            | Recursos computacionais restritos  | Clusters escalÃ¡veis sob demanda        |
| **SeguranÃ§a**                 | BÃ¡sica, sem integraÃ§Ã£o corporativa | IntegraÃ§Ã£o com Azure AD, RBAC, redes   |
| **IntegraÃ§Ã£o**                | Sem integraÃ§Ã£o com outros serviÃ§os | IntegraÃ§Ã£o total com Azure (Data Lake, Synapse, Key Vault, etc.) |
| **PersistÃªncia de Dados**     | Dados podem ser apagados           | PersistÃªncia garantida em Blob/ADLS    |
| **Agendamento de Jobs**       | Limitado, sem triggers avanÃ§adas   | Agendamento flexÃ­vel e triggers        |
| **Limite de UsuÃ¡rios**        | Apenas 1 usuÃ¡rio                   | MultiusuÃ¡rio, colaboraÃ§Ã£o corporativa  |
| **Suporte e SLA**             | Sem suporte oficial                | Suporte Microsoft, SLA corporativo     |
| **Recursos AvanÃ§ados**        | NÃ£o disponÃ­vel                     | MLflow, Delta Sharing, Unity Catalog   |

> **Resumo:**
> - O Free Edition Ã© ideal para testes, prototipagem e aprendizado, mas possui limitaÃ§Ãµes de recursos, armazenamento e integraÃ§Ã£o.
> - O Azure Databricks Ã© recomendado para ambientes produtivos, com escalabilidade, seguranÃ§a, integraÃ§Ã£o corporativa e persistÃªncia de dados garantida.


